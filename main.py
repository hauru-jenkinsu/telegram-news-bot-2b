import feedparser
import telegram
import time
import json
import logging
import os
import re
import schedule
import asyncio
from urllib.parse import urlparse, urlunparse, parse_qs, urlencode

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
log_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'parser.log')
logging.basicConfig(
    filename=log_file,
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logging.getLogger().addHandler(logging.StreamHandler())

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
TOKEN = os.getenv("TOKEN")
CHANNELS = os.getenv("CHANNELS", "").split(",")
logging.info(f"–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è CHANNELS: {CHANNELS}")
ADMIN_CHAT_ID = os.getenv("ADMIN_CHAT_ID")

RSS_FEEDS = [
    {"name": "lenta.ru", "url": "https://lenta.ru/rss/news/", "fallback": "https://lenta.ru/rss/"},
    {"name": "vpk.name", "url": "https://vpk.name/rss/", "fallback": None},
    {"name": "ria.ru", "url": "https://ria.ru/export/rss2/index.xml", "fallback": "https://ria.ru/export/rss2/army/index.xml"},
    {"name": "rg.ru", "url": "https://rg.ru/xml/index.xml", "fallback": None},
    {"name": "tass.ru", "url": "https://tass.ru/rss/v2.xml?sections=MjQ%3D", "fallback": None}
]

KEYWORDS = [
    "—Å–≤–æ", "–≤—Å —Ä—Ñ", "—Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –≤–æ–π—Å–∫–∞", "—Ä–æ—Å—Å–∏–π—Å–∫–∏–µ —É—á–µ–Ω–∏—è", "—Ä–æ—Å—Å–∏–π—Å–∫–∞—è –∞—Ä–º–∏—è",
    "—Ä–æ—Å—Å–∏–π—Å–∫–æ–µ –≤–æ–æ—Ä—É–∂–µ–Ω–∏–µ", "—Ä–æ—Å—Å–∏–π—Å–∫–∞—è —Ç–µ—Ö–Ω–∏–∫–∞", "—Å–ø–µ—Ü–æ–ø–µ—Ä–∞—Ü–∏—è", "–≤–æ–µ–Ω–Ω—ã–π",
    "–∞—Ä–º–∏—è", "—É—á–µ–Ω–∏—è", "–≤–æ–π—Å–∫–∞", "—Ç–µ—Ö–Ω–∏–∫–∞", "–æ–¥–∫–±", "–≤–æ–æ—Ä—É–∂–µ–Ω–∏–µ",
    "—Ç–∞–Ω–∫", "—Ñ–ª–æ—Ç", "—Ä–∞–∫–µ—Ç–∞", "–æ–±–æ—Ä–æ–Ω–∞", "–º–∏–Ω–æ–±–æ—Ä–æ–Ω—ã", "–±–µ–ª–æ—É—Å–æ–≤",
    "–≤–æ–æ—Ä—É–∂—ë–Ω–Ω—ã–µ —Å–∏–ª—ã", "–∫–æ–Ω—Ñ–ª–∏–∫—Ç", "–Ω–∞—ë–º–Ω–∏–∫", "–≤—Å—É"
]

PROCESSED_LINKS_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'processed_links.json')
PROCESSED_TITLES_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'processed_titles.json')
REJECTED_NEWS_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'rejected_news.json')
TEST_MODE = False  # True ‚Äî –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –±–µ–∑ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞
try:
    bot = telegram.Bot(token=TOKEN)
    logging.info("–ë–æ—Ç —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
except Exception as e:
    logging.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–æ—Ç–∞: {e}")
    exit(1)

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
def load_processed_links():
    try:
        with open(PROCESSED_LINKS_FILE, "r", encoding='utf-8') as f:
            return set(json.load(f))
    except FileNotFoundError:
        logging.info(f"–§–∞–π–ª {PROCESSED_LINKS_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π")
        return set()

def save_processed_links(links):
    try:
        with open(PROCESSED_LINKS_FILE, "w", encoding='utf-8') as f:
            json.dump(list(links), f, ensure_ascii=False)
        logging.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫ –≤ {PROCESSED_LINKS_FILE}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫: {e}")

def load_processed_titles():
    try:
        with open(PROCESSED_TITLES_FILE, "r", encoding='utf-8') as f:
            return set(json.load(f))
    except FileNotFoundError:
        logging.info(f"–§–∞–π–ª {PROCESSED_TITLES_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π")
        return set()

def save_processed_titles(titles):
    try:
        with open(PROCESSED_TITLES_FILE, "w", encoding='utf-8') as f:
            json.dump(list(titles), f, ensure_ascii=False)
        logging.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(titles)} –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤ {PROCESSED_TITLES_FILE}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤: {e}")

def save_rejected_news(title, link, reason):
    try:
        rejected = []
        if os.path.exists(REJECTED_NEWS_FILE):
            with open(REJECTED_NEWS_FILE, "r", encoding='utf-8') as f:
                rejected = json.load(f)
        rejected.append({"title": title, "link": link, "reason": reason, "time": time.strftime("%Y-%m-%d %H:%M:%S")})
        with open(REJECTED_NEWS_FILE, "w", encoding='utf-8') as f:
            json.dump(rejected, f, ensure_ascii=False, indent=2)
        logging.debug(f"–û—Ç–∫–ª–æ–Ω–µ–Ω–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {title}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç–∫–ª–æ–Ω–µ–Ω–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏: {e}")

def normalize_url(url):
    try:
        parsed = urlparse(url)
        clean_query = {k: v for k, v in parse_qs(parsed.query).items() if not k.startswith('utm_')}
        new_query = urlencode(clean_query, doseq=True)
        cleaned = parsed._replace(query=new_query, fragment='')
        return urlunparse(cleaned)
    except Exception as e:
        logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å—Å—ã–ª–∫—É: {url} ‚Äî {e}")
        return url

async def parse_rss(feed):
    try:
        feed_data = feedparser.parse(feed["url"])
        if not feed_data.entries and feed["fallback"]:
            logging.warning(f"–ü—É—Å—Ç–∞—è RSS-–ª–µ–Ω—Ç–∞: {feed['url']}, –ø—Ä–æ–±—É–µ–º –∑–∞–ø–∞—Å–Ω–æ–π URL: {feed['fallback']}")
            feed_data = feedparser.parse(feed["fallback"])
        if not feed_data.entries:
            logging.warning(f"–ü—É—Å—Ç–∞—è RSS-–ª–µ–Ω—Ç–∞: {feed['name']} ({feed['url']})")
            await bot.send_message(chat_id=ADMIN_CHAT_ID, text=f"–ü—É—Å—Ç–∞—è RSS-–ª–µ–Ω—Ç–∞: {feed['name']} ({feed['url']})")
            return []
        news = []
        for entry in feed_data.entries[:5]:
            title = entry.get("title", "").strip()
            link = entry.get("link", "").strip()
            desc = entry.get("summary", entry.get("description", "")).strip()
            if title and link:
                news.append({"title": title, "link": link, "desc": desc})
        logging.info(f"–°–ø–∞—Ä—Å–µ–Ω–æ {feed['name']}: {len(news)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        return news
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {feed['name']}: {e}")
        await bot.send_message(chat_id=ADMIN_CHAT_ID, text=f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS {feed['name']}: {e}")
        return []

async def publish_news(title, link):
    message = f"üì∞ {title}\nüîó {link}"
    if TEST_MODE:
        logging.info(f"[–¢–ï–°–¢] –ù–æ–≤–æ—Å—Ç—å –≥–æ—Ç–æ–≤–∞ –∫ –æ—Ç–ø—Ä–∞–≤–∫–µ: {message}")
        print(f"[–¢–ï–°–¢] {message}")
        return True

    success = True
    for channel in CHANNELS:
        if not channel.strip():
            logging.warning("–ü—Ä–æ–ø—É—â–µ–Ω –ø—É—Å—Ç–æ–π chat_id –≤ —Å–ø–∏—Å–∫–µ CHANNELS")
            continue
        try:
            await bot.send_message(chat_id=channel.strip(), text=message)
            await asyncio.sleep(2)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ {channel}: {e}")
            success = False
            if ADMIN_CHAT_ID:
                try:
                    await bot.send_message(chat_id=ADMIN_CHAT_ID, text=f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ {channel}: {e}")
                except Exception as e_admin:
                    logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø–æ–≤–µ—â–µ–Ω–∏–∏ –∞–¥–º–∏–Ω–∞: {e_admin}")
    return success

def matches_keywords(text):
    text = text.lower()
    for keyword in KEYWORDS:
        if re.search(r'\b' + re.escape(keyword.lower()) + r'\b', text):
            return True
    return False

async def main():
    logging.info(f"–ó–∞–ø—É—Å–∫: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    processed_links = load_processed_links()
    processed_titles = load_processed_titles()

    for feed in RSS_FEEDS:
        news = await parse_rss(feed)
        for item in news:
            title = item["title"]
            desc = item["desc"]
            link = item["link"]
            normalized_link = normalize_url(link)

            if normalized_link in processed_links or title in processed_titles:
                save_rejected_news(title, link, "–¥—É–±–ª–∏–∫–∞—Ç")
                continue

            if matches_keywords(title) or matches_keywords(desc):
                if await publish_news(title, link):
                    processed_links.add(normalized_link)
                    processed_titles.add(title)
                else:
                    save_rejected_news(title, link, "–æ—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏")
            else:
                save_rejected_news(title, link, "–Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º")

    save_processed_links(processed_links)
    save_processed_titles(processed_titles)
    logging.info("–ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω")

# –î–ª—è –æ—Ç–ª–∞–¥–∫–∏:
if __name__ == "__main__":
    asyncio.run(main())
